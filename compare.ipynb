{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57e57398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.sd_gru as sd_gru\n",
    "import models.min_gru as min_gru\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0beba000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8f4e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc='''Recurrent neural networks (RNNs) are deep learning models, typically used to solve problems with sequential input data such as time series. What are they, and how do we use them in time series forecasting?\n",
    "\n",
    "RNNs are a type of neural network that retains a memory of what it has already processed and thus can learn from previous iterations during its training.\n",
    "\n",
    "Probably you have done what most of us do when we hear any technical term for the first time. You have tried to understand what recurrent neural networks are by clicking on the top-listed non-ad Google search result. Then you will have found that Wikipedia’s article exhibits a high level of abstraction. It is of limited usefulness when we try to understand what RNNs are and what they are for: \"A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs …. Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.\" Say what?\n",
    "\n",
    "Michael Phi provided an excellent, non-mathematical guide on RNNs in a previous Towards Data Science article of his: \"Illustrated Guide to Recurrent Neural Networks | by Michael Phi | Towards Data Science\". So did Will Koehrsen, in \"Recurrent Neural Networks by Example in Python | by Will Koehrsen | Towards Data Science.\"\n",
    "\n",
    "Let me summarize the basics we should understand about RNNs, in non-mathematical terms (and then I’d refer you to the additional explanations and illustrations in the two articles Michael and Will wrote in 2018).\n",
    "\n",
    "A neural network – of which recurrent neural networks are one type, among other types such as convolutional networks – is composed of three elementary components: the input layer, the hidden layers, and the output layer. Each layer consists of so-called nodes (aka neurons).\n",
    "\n",
    "I’ve read the following analogy for the three main types of neural networks, which are said to mimic human brain functions in specific ways. The following comparisons oversimplify, so best take them with a grain of salt.\n",
    "\n",
    "the temporal lobe of our brain => artificial neural networks => mainly for classification and regression problems => one of the functions of the temporal lobe is long-term memory\n",
    "the occipital lobe => convolutional neural networks => mainly for computer vision problems (though temporal convolutional networks, TCNs, can be applied to time series)\n",
    "the frontal lobe => recurrent neural networks RNN => mainly for time series analysis, sequences, and lists – for instance, in language processing, which deals with sequences of characters, words, and sentences ordered by a grammar; or time series, which consist of temporal sequences of observations => one of the frontal lobe’s functions is short-term memory\n",
    "Feed-forward neural networks (FFNNs) – such as the grandfather among neural networks, the original single-layer perceptron, developed in 1958— came before recurrent neural networks. In FFNNs, the information flows in only one direction: from the input layer, through the hidden layers, to the output layer, but never backwards in feedback loops. FFNN are often used in pattern recognition. The FFNN multiplies a matrix of weight factors with the inputs and generates the outputs from these weighted inputs. Feed-forward neural networks don’t retain a memory of the inputs they have processed. They suffer from anterograde amnesia, the inability to form new memories (similar to the protagonist in Christopher Nolan’s movie Memento – Wikipedia [this seemed a rare opportunity to mention anterograde amnesia and Memento in a data science article]).\n",
    "\n",
    "A recurrent neural network, by contrast, retains a memory of what it has processed in its recent previous steps (we’ll come back to the \"recent\" qualifier in a minute). It makes recurrent connections by going through temporal feedback loops: the output of a preceding step is used as an input for the current process step. Unlike amnesiac FFNNs, this memory enables RNNs to process sequences of inputs without loosing track. The loops make it a recurrent network.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cf6ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([doc])\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding/indexing\n",
    "sequences = tokenizer.texts_to_sequences([doc])[0]\n",
    "\n",
    "# Create input-output pairs for training\n",
    "X_train, Y_train = [], []\n",
    "seq_length = 3  # Number of words to predict the next word\n",
    "\n",
    "for i in range(len(sequences) - seq_length):\n",
    "    X_train.append(sequences[i:i+seq_length])\n",
    "    Y_train.append(sequences[i+seq_length])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = to_categorical(Y_train, num_classes=vocab_size)  # Convert output to one-hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dba59f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = sd_gru.train_gru_model(X_train, Y_train, vocab_size,epochs=500)\n",
    "# word = sd_gru.predict_next_word_gru(model, tokenizer, \"recurrent neural networks\")\n",
    "# print(\"Predicted:\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0522a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_gru_model, emb_layer = min_gru.train_min_gru_parallel(X_train, Y_train, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9993208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_text = \"recurrent neural networks\"\n",
    "# predicted = min_gru.predict_min_gru_parallel(min_gru_model, tokenizer, input_text, emb_layer, hidden_size=10)\n",
    "# print(f\"Input: '{input_text}' → Predicted next word: '{predicted}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c338adc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_gru_model() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# === Training Standard GRU ===\u001b[39;00m\n\u001b[0;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 3\u001b[0m std_model \u001b[38;5;241m=\u001b[39m \u001b[43msd_gru\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_gru_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m std_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Standard GRU] Training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: train_gru_model() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Training Standard GRU ===\n",
    "start_time = time.time()\n",
    "std_model = sd_gru.train_gru_model(X_train, Y_train, vocab_size,verbose=False,epochs=500)\n",
    "std_time = time.time() - start_time\n",
    "print(f\"[Standard GRU] Training time: {std_time:.2f} seconds\")\n",
    "\n",
    "# === Training MinGRU ===\n",
    "start_time = time.time()\n",
    "min_model, embed_layer = min_gru.train_min_gru_parallel(X_train, Y_train, vocab_size,verbose=False,epochs=500)\n",
    "min_time = time.time() - start_time\n",
    "print(f\"[MinGRU] Training time: {min_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed047c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
