{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kD8BMHKmySq_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zyngVCCp0NEX"
      },
      "outputs": [],
      "source": [
        "doc='''Recurrent neural networks (RNNs) are deep learning models, typically used to solve problems with sequential input data such as time series. What are they, and how do we use them in time series forecasting?\n",
        "\n",
        "RNNs are a type of neural network that retains a memory of what it has already processed and thus can learn from previous iterations during its training.\n",
        "\n",
        "Probably you have done what most of us do when we hear any technical term for the first time. You have tried to understand what recurrent neural networks are by clicking on the top-listed non-ad Google search result. Then you will have found that Wikipedia’s article exhibits a high level of abstraction. It is of limited usefulness when we try to understand what RNNs are and what they are for: \"A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs …. Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.\" Say what?\n",
        "\n",
        "Michael Phi provided an excellent, non-mathematical guide on RNNs in a previous Towards Data Science article of his: \"Illustrated Guide to Recurrent Neural Networks | by Michael Phi | Towards Data Science\". So did Will Koehrsen, in \"Recurrent Neural Networks by Example in Python | by Will Koehrsen | Towards Data Science.\"\n",
        "\n",
        "Let me summarize the basics we should understand about RNNs, in non-mathematical terms (and then I’d refer you to the additional explanations and illustrations in the two articles Michael and Will wrote in 2018).\n",
        "\n",
        "A neural network – of which recurrent neural networks are one type, among other types such as convolutional networks – is composed of three elementary components: the input layer, the hidden layers, and the output layer. Each layer consists of so-called nodes (aka neurons).\n",
        "\n",
        "I’ve read the following analogy for the three main types of neural networks, which are said to mimic human brain functions in specific ways. The following comparisons oversimplify, so best take them with a grain of salt.\n",
        "\n",
        "the temporal lobe of our brain => artificial neural networks => mainly for classification and regression problems => one of the functions of the temporal lobe is long-term memory\n",
        "the occipital lobe => convolutional neural networks => mainly for computer vision problems (though temporal convolutional networks, TCNs, can be applied to time series)\n",
        "the frontal lobe => recurrent neural networks RNN => mainly for time series analysis, sequences, and lists – for instance, in language processing, which deals with sequences of characters, words, and sentences ordered by a grammar; or time series, which consist of temporal sequences of observations => one of the frontal lobe’s functions is short-term memory\n",
        "Feed-forward neural networks (FFNNs) – such as the grandfather among neural networks, the original single-layer perceptron, developed in 1958— came before recurrent neural networks. In FFNNs, the information flows in only one direction: from the input layer, through the hidden layers, to the output layer, but never backwards in feedback loops. FFNN are often used in pattern recognition. The FFNN multiplies a matrix of weight factors with the inputs and generates the outputs from these weighted inputs. Feed-forward neural networks don’t retain a memory of the inputs they have processed. They suffer from anterograde amnesia, the inability to form new memories (similar to the protagonist in Christopher Nolan’s movie Memento – Wikipedia [this seemed a rare opportunity to mention anterograde amnesia and Memento in a data science article]).\n",
        "\n",
        "A recurrent neural network, by contrast, retains a memory of what it has processed in its recent previous steps (we’ll come back to the \"recent\" qualifier in a minute). It makes recurrent connections by going through temporal feedback loops: the output of a preceding step is used as an input for the current process step. Unlike amnesiac FFNNs, this memory enables RNNs to process sequences of inputs without loosing track. The loops make it a recurrent network.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atH8GEt41xMp"
      },
      "source": [
        "## data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okqtwe-yQ9uR"
      },
      "source": [
        "## coding LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D6kUe9YEfM8I"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sGClPbp1fK2S"
      },
      "outputs": [],
      "source": [
        "# Tokenize words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([doc])\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding/indexing\n",
        "sequences = tokenizer.texts_to_sequences([doc])[0]\n",
        "\n",
        "# Create input-output pairs for training\n",
        "X_train, Y_train = [], []\n",
        "seq_length = 3  # Number of words to predict the next word\n",
        "\n",
        "for i in range(len(sequences) - seq_length):\n",
        "    X_train.append(sequences[i:i+seq_length])\n",
        "    Y_train.append(sequences[i+seq_length])\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = to_categorical(Y_train, num_classes=vocab_size)  # Convert output to one-hot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  9,   3,   5],\n",
              "       [  3,   5,  13],\n",
              "       [  5,  13,  10],\n",
              "       ...,\n",
              "       [ 60, 298,  18],\n",
              "       [298,  18,   4],\n",
              "       [ 18,   4,   9]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW_9yOhQgDr0",
        "outputId": "94ac6a97-63d2-4365-e8c4-4f3a33328cb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((679, 3), (679, 299), 299)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape,Y_train.shape,vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ImVF9AymMjv"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NpGjyvR2w56P"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define LSTM cell\n",
        "class LSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dense_gate = tf.keras.layers.Dense(hidden_size * 4, activation=None, use_bias=True)\n",
        "        self.dense_output = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")  # Softmax for word prediction\n",
        "\n",
        "    def call(self, x, h_prev, c_prev):\n",
        "        concat_input = tf.concat([x, h_prev], axis=1)  # Keep axis=1 for batch processing\n",
        "        gates = self.dense_gate(concat_input)\n",
        "\n",
        "        f_gate, i_gate, o_gate, g_gate = tf.split(gates, num_or_size_splits=4, axis=-1)\n",
        "\n",
        "        f_gate = tf.sigmoid(f_gate)\n",
        "        i_gate = tf.sigmoid(i_gate)\n",
        "        o_gate = tf.sigmoid(o_gate)\n",
        "        g_gate = tf.tanh(g_gate)\n",
        "\n",
        "        c_next = f_gate * c_prev + i_gate * g_gate\n",
        "        h_next = o_gate * tf.tanh(c_next)\n",
        "        y = self.dense_output(h_next)\n",
        "\n",
        "        return h_next, c_next, y\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3d1HG6-Y12OV"
      },
      "outputs": [],
      "source": [
        "# Training function for a custom LSTMCell model\n",
        "def train(X, Y, hidden_size=10, epochs=500):\n",
        "    \n",
        "    # Initialize the Adam optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "    # Create an instance of your custom LSTM cell (defined earlier)\n",
        "    lstm_cell = LSTMCell(hidden_size, vocab_size)\n",
        "\n",
        "    # Define the word embedding layer that converts token indices to dense vectors\n",
        "    embedding_dim = 8  # Size of word embeddings\n",
        "    embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Initialize hidden and cell states to zeros for each batch\n",
        "            h_prev = tf.zeros((X.shape[0], hidden_size), dtype=tf.float32)  # shape: [batch_size, hidden_size]\n",
        "            c_prev = tf.zeros((X.shape[0], hidden_size), dtype=tf.float32)\n",
        "            loss = 0  # initialize loss\n",
        "\n",
        "            # Embed input word indices into dense vectors\n",
        "            embedded_X = embedding_layer(X)  # shape: [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "            # Loop through the time steps (i.e., sequence length)\n",
        "            for t in range(seq_length):\n",
        "                x_t = embedded_X[:, t, :]  # extract embedding for t-th word in sequence\n",
        "                h_prev, c_prev, y_t = lstm_cell(x_t, h_prev, c_prev)  # run LSTM cell step\n",
        "\n",
        "            # Compute categorical crossentropy loss between predicted distribution and ground truth\n",
        "            loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(Y, y_t))  # loss on final output\n",
        "\n",
        "        # Compute gradients of loss w.r.t. LSTM cell and embedding layer parameters\n",
        "        gradients = tape.gradient(loss, lstm_cell.trainable_variables + embedding_layer.trainable_variables)\n",
        "\n",
        "        # Apply gradients to update the weights\n",
        "        optimizer.apply_gradients(zip(gradients, lstm_cell.trainable_variables + embedding_layer.trainable_variables))\n",
        "\n",
        "        # Optionally print loss every 10 epochs\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
        "\n",
        "        # On the last epoch, return the trained LSTM cell and embedding layer\n",
        "        if epoch == epochs - 1:\n",
        "            return lstm_cell, embedding_layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9h6HyXSTfjQb",
        "outputId": "d4af60d2-703b-4c42-fea0-4f2e028655ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 5.70045280456543\n",
            "Epoch 10, Loss: 5.559117794036865\n",
            "Epoch 20, Loss: 5.1533942222595215\n",
            "Epoch 30, Loss: 4.954916477203369\n",
            "Epoch 40, Loss: 4.762920379638672\n",
            "Epoch 50, Loss: 4.494144439697266\n",
            "Epoch 60, Loss: 4.206137180328369\n",
            "Epoch 70, Loss: 3.912172555923462\n",
            "Epoch 80, Loss: 3.6127102375030518\n",
            "Epoch 90, Loss: 3.299605131149292\n",
            "Epoch 100, Loss: 2.9815399646759033\n",
            "Epoch 110, Loss: 2.6761324405670166\n",
            "Epoch 120, Loss: 2.393909215927124\n",
            "Epoch 130, Loss: 2.1423416137695312\n",
            "Epoch 140, Loss: 1.9190465211868286\n",
            "Epoch 150, Loss: 1.7267868518829346\n",
            "Epoch 160, Loss: 1.5622824430465698\n",
            "Epoch 170, Loss: 1.419525384902954\n",
            "Epoch 180, Loss: 1.2973625659942627\n",
            "Epoch 190, Loss: 1.1918106079101562\n",
            "Epoch 200, Loss: 1.1000354290008545\n",
            "Epoch 210, Loss: 1.0176762342453003\n",
            "Epoch 220, Loss: 0.9413213729858398\n",
            "Epoch 230, Loss: 0.8683884143829346\n",
            "Epoch 240, Loss: 0.8026047348976135\n",
            "Epoch 250, Loss: 0.7456746101379395\n",
            "Epoch 260, Loss: 0.6946704387664795\n",
            "Epoch 270, Loss: 0.6503537893295288\n",
            "Epoch 280, Loss: 0.610873281955719\n",
            "Epoch 290, Loss: 0.5762520432472229\n",
            "Epoch 300, Loss: 0.5453776717185974\n",
            "Epoch 310, Loss: 0.5168682932853699\n",
            "Epoch 320, Loss: 0.49106860160827637\n",
            "Epoch 330, Loss: 0.4679335355758667\n",
            "Epoch 340, Loss: 0.44715672731399536\n",
            "Epoch 350, Loss: 0.42840054631233215\n",
            "Epoch 360, Loss: 0.41209831833839417\n",
            "Epoch 370, Loss: 0.39523574709892273\n",
            "Epoch 380, Loss: 0.37985867261886597\n",
            "Epoch 390, Loss: 0.36653754115104675\n",
            "Epoch 400, Loss: 0.35376936197280884\n",
            "Epoch 410, Loss: 0.34178587794303894\n",
            "Epoch 420, Loss: 0.3306591212749481\n",
            "Epoch 430, Loss: 0.3196409046649933\n",
            "Epoch 440, Loss: 0.30978551506996155\n",
            "Epoch 450, Loss: 0.3005184829235077\n",
            "Epoch 460, Loss: 0.2916409969329834\n",
            "Epoch 470, Loss: 0.283218115568161\n",
            "Epoch 480, Loss: 0.2757263779640198\n",
            "Epoch 490, Loss: 0.26910924911499023\n"
          ]
        }
      ],
      "source": [
        "model,emb_layer=train(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ca-AYeaty3oZ"
      },
      "outputs": [],
      "source": [
        "# emb=emb_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qT_0U2V4oeNd"
      },
      "outputs": [],
      "source": [
        "def predict_next_word(model, tokenizer, input_text, hidden_size):\n",
        "    \"\"\"\n",
        "    Predicts the next word given an input sequence.\n",
        "\n",
        "    Parameters:\n",
        "        model: The trained LSTMCell model.\n",
        "        tokenizer: The tokenizer used for text preprocessing.\n",
        "        input_text: A string containing the input sequence.\n",
        "        hidden_size: The size of the hidden state of LSTM.\n",
        "\n",
        "    Returns:\n",
        "        Predicted next word as a string.\n",
        "    \"\"\"\n",
        "    # Convert input text to sequence\n",
        "    sequence = tokenizer.texts_to_sequences([input_text])[0]\n",
        "\n",
        "    # Ensure sequence length matches training input length\n",
        "    seq_length = 3  # Same as in training\n",
        "    if len(sequence) < seq_length:\n",
        "        print(\"Input sequence is too short!\")\n",
        "        return None\n",
        "    sequence = sequence[-seq_length:]  # Take the last seq_length words i.e only 3\n",
        "\n",
        "    # Convert to NumPy array\n",
        "    X_input = np.array([sequence])\n",
        "\n",
        "    # Initialize hidden and cell states\n",
        "    h_prev = tf.zeros((1, hidden_size), dtype=tf.float32)\n",
        "    c_prev = tf.zeros((1, hidden_size), dtype=tf.float32)\n",
        "\n",
        "    # Get word embeddings\n",
        "    embedded_X = emb_layer(X_input)\n",
        "\n",
        "    # Pass through LSTM cell\n",
        "    for t in range(seq_length):\n",
        "        x_t = embedded_X[:, t, :]\n",
        "        h_prev, c_prev, y_t = model(x_t, h_prev, c_prev)\n",
        "\n",
        "    # Get predicted word index\n",
        "    predicted_index = tf.argmax(y_t, axis=1).numpy()[0]\n",
        "\n",
        "    # Convert index to word\n",
        "    predicted_word = tokenizer.index_word.get(predicted_index, \"<UNK>\")\n",
        "\n",
        "    return predicted_word\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epHaTIhVlAMv",
        "outputId": "23c84d6b-ec4f-47c5-bf9b-dee75bc42eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted next word: in\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_text = \" we should understand about RNNs\"\n",
        "predicted_word = predict_next_word(model, tokenizer, input_text, hidden_size=10)\n",
        "print(f\"Predicted next word: {predicted_word}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "7x7NWcOwyqBT",
        "outputId": "276dbb60-d217-4427-c357-20b3d2f6ef21"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Recurrent neural networks (RNNs) are deep learning models, typically used to solve problems with sequential input data such as time series. What are they, and how do we use them in time series forecasting?\\n\\nRNNs are a type of neural network that retains a memory of what it has already processed and thus can learn from previous iterations during its training.\\n\\nProbably you have done what most of us do when we hear any technical term for the first time. You have tried to understand what recurrent neural networks are by clicking on the top-listed non-ad Google search result. Then you will have found that Wikipedia’s article exhibits a high level of abstraction. It is of limited usefulness when we try to understand what RNNs are and what they are for: \"A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs …. Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.\" Say what?\\n\\nMichael Phi provided an excellent, non-mathematical guide on RNNs in a previous Towards Data Science article of his: \"Illustrated Guide to Recurrent Neural Networks | by Michael Phi | Towards Data Science\". So did Will Koehrsen, in \"Recurrent Neural Networks by Example in Python | by Will Koehrsen | Towards Data Science.\"\\n\\nLet me summarize the basics we should understand about RNNs, in non-mathematical terms (and then I’d refer you to the additional explanations and illustrations in the two articles Michael and Will wrote in 2018).\\n\\nA neural network – of which recurrent neural networks are one type, among other types such as convolutional networks – is composed of three elementary components: the input layer, the hidden layers, and the output layer. Each layer consists of so-called nodes (aka neurons).\\n\\nI’ve read the following analogy for the three main types of neural networks, which are said to mimic human brain functions in specific ways. The following comparisons oversimplify, so best take them with a grain of salt.\\n\\nthe temporal lobe of our brain => artificial neural networks => mainly for classification and regression problems => one of the functions of the temporal lobe is long-term memory\\nthe occipital lobe => convolutional neural networks => mainly for computer vision problems (though temporal convolutional networks, TCNs, can be applied to time series)\\nthe frontal lobe => recurrent neural networks RNN => mainly for time series analysis, sequences, and lists – for instance, in language processing, which deals with sequences of characters, words, and sentences ordered by a grammar; or time series, which consist of temporal sequences of observations => one of the frontal lobe’s functions is short-term memory\\nFeed-forward neural networks (FFNNs) – such as the grandfather among neural networks, the original single-layer perceptron, developed in 1958— came before recurrent neural networks. In FFNNs, the information flows in only one direction: from the input layer, through the hidden layers, to the output layer, but never backwards in feedback loops. FFNN are often used in pattern recognition. The FFNN multiplies a matrix of weight factors with the inputs and generates the outputs from these weighted inputs. Feed-forward neural networks don’t retain a memory of the inputs they have processed. They suffer from anterograde amnesia, the inability to form new memories (similar to the protagonist in Christopher Nolan’s movie Memento – Wikipedia [this seemed a rare opportunity to mention anterograde amnesia and Memento in a data science article]).\\n\\nA recurrent neural network, by contrast, retains a memory of what it has processed in its recent previous steps (we’ll come back to the \"recent\" qualifier in a minute). It makes recurrent connections by going through temporal feedback loops: the output of a preceding step is used as an input for the current process step. Unlike amnesiac FFNNs, this memory enables RNNs to process sequences of inputs without loosing track. The loops make it a recurrent network.'"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''Recurrent neural networks (RNNs) are deep learning models, typically used to solve problems with sequential input data such as time series. What are they, and how do we use them in time series forecasting?\n",
        "\n",
        "RNNs are a type of neural network that retains a memory of what it has already processed and thus can learn from previous iterations during its training.\n",
        "\n",
        "Probably you have done what most of us do when we hear any technical term for the first time. You have tried to understand what recurrent neural networks are by clicking on the top-listed non-ad Google search result. Then you will have found that Wikipedia’s article exhibits a high level of abstraction. It is of limited usefulness when we try to understand what RNNs are and what they are for: \"A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs …. Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.\" Say what?\n",
        "\n",
        "Michael Phi provided an excellent, non-mathematical guide on RNNs in a previous Towards Data Science article of his: \"Illustrated Guide to Recurrent Neural Networks | by Michael Phi | Towards Data Science\". So did Will Koehrsen, in \"Recurrent Neural Networks by Example in Python | by Will Koehrsen | Towards Data Science.\"\n",
        "\n",
        "Let me summarize the basics we should understand about RNNs, in non-mathematical terms (and then I’d refer you to the additional explanations and illustrations in the two articles Michael and Will wrote in 2018).\n",
        "\n",
        "A neural network – of which recurrent neural networks are one type, among other types such as convolutional networks – is composed of three elementary components: the input layer, the hidden layers, and the output layer. Each layer consists of so-called nodes (aka neurons).\n",
        "\n",
        "I’ve read the following analogy for the three main types of neural networks, which are said to mimic human brain functions in specific ways. The following comparisons oversimplify, so best take them with a grain of salt.\n",
        "\n",
        "the temporal lobe of our brain => artificial neural networks => mainly for classification and regression problems => one of the functions of the temporal lobe is long-term memory\n",
        "the occipital lobe => convolutional neural networks => mainly for computer vision problems (though temporal convolutional networks, TCNs, can be applied to time series)\n",
        "the frontal lobe => recurrent neural networks RNN => mainly for time series analysis, sequences, and lists – for instance, in language processing, which deals with sequences of characters, words, and sentences ordered by a grammar; or time series, which consist of temporal sequences of observations => one of the frontal lobe’s functions is short-term memory\n",
        "Feed-forward neural networks (FFNNs) – such as the grandfather among neural networks, the original single-layer perceptron, developed in 1958— came before recurrent neural networks. In FFNNs, the information flows in only one direction: from the input layer, through the hidden layers, to the output layer, but never backwards in feedback loops. FFNN are often used in pattern recognition. The FFNN multiplies a matrix of weight factors with the inputs and generates the outputs from these weighted inputs. Feed-forward neural networks don’t retain a memory of the inputs they have processed. They suffer from anterograde amnesia, the inability to form new memories (similar to the protagonist in Christopher Nolan’s movie Memento – Wikipedia [this seemed a rare opportunity to mention anterograde amnesia and Memento in a data science article]).\n",
        "\n",
        "A recurrent neural network, by contrast, retains a memory of what it has processed in its recent previous steps (we’ll come back to the \"recent\" qualifier in a minute). It makes recurrent connections by going through temporal feedback loops: the output of a preceding step is used as an input for the current process step. Unlike amnesiac FFNNs, this memory enables RNNs to process sequences of inputs without loosing track. The loops make it a recurrent network.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqXQYI7azmRB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "python_mlops",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
