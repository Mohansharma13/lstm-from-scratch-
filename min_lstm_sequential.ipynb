{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89253cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0999ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc='''Recurrent neural networks (RNNs) are deep learning models, typically used to solve problems with sequential input data such as time series. What are they, and how do we use them in time series forecasting?\n",
    "\n",
    "RNNs are a type of neural network that retains a memory of what it has already processed and thus can learn from previous iterations during its training.\n",
    "\n",
    "Probably you have done what most of us do when we hear any technical term for the first time. You have tried to understand what recurrent neural networks are by clicking on the top-listed non-ad Google search result. Then you will have found that Wikipedia’s article exhibits a high level of abstraction. It is of limited usefulness when we try to understand what RNNs are and what they are for: \"A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs …. Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.\" Say what?\n",
    "\n",
    "Michael Phi provided an excellent, non-mathematical guide on RNNs in a previous Towards Data Science article of his: \"Illustrated Guide to Recurrent Neural Networks | by Michael Phi | Towards Data Science\". So did Will Koehrsen, in \"Recurrent Neural Networks by Example in Python | by Will Koehrsen | Towards Data Science.\"\n",
    "\n",
    "Let me summarize the basics we should understand about RNNs, in non-mathematical terms (and then I’d refer you to the additional explanations and illustrations in the two articles Michael and Will wrote in 2018).\n",
    "\n",
    "A neural network – of which recurrent neural networks are one type, among other types such as convolutional networks – is composed of three elementary components: the input layer, the hidden layers, and the output layer. Each layer consists of so-called nodes (aka neurons).\n",
    "\n",
    "I’ve read the following analogy for the three main types of neural networks, which are said to mimic human brain functions in specific ways. The following comparisons oversimplify, so best take them with a grain of salt.\n",
    "\n",
    "the temporal lobe of our brain => artificial neural networks => mainly for classification and regression problems => one of the functions of the temporal lobe is long-term memory\n",
    "the occipital lobe => convolutional neural networks => mainly for computer vision problems (though temporal convolutional networks, TCNs, can be applied to time series)\n",
    "the frontal lobe => recurrent neural networks RNN => mainly for time series analysis, sequences, and lists – for instance, in language processing, which deals with sequences of characters, words, and sentences ordered by a grammar; or time series, which consist of temporal sequences of observations => one of the frontal lobe’s functions is short-term memory\n",
    "Feed-forward neural networks (FFNNs) – such as the grandfather among neural networks, the original single-layer perceptron, developed in 1958— came before recurrent neural networks. In FFNNs, the information flows in only one direction: from the input layer, through the hidden layers, to the output layer, but never backwards in feedback loops. FFNN are often used in pattern recognition. The FFNN multiplies a matrix of weight factors with the inputs and generates the outputs from these weighted inputs. Feed-forward neural networks don’t retain a memory of the inputs they have processed. They suffer from anterograde amnesia, the inability to form new memories (similar to the protagonist in Christopher Nolan’s movie Memento – Wikipedia [this seemed a rare opportunity to mention anterograde amnesia and Memento in a data science article]).\n",
    "\n",
    "A recurrent neural network, by contrast, retains a memory of what it has processed in its recent previous steps (we’ll come back to the \"recent\" qualifier in a minute). It makes recurrent connections by going through temporal feedback loops: the output of a preceding step is used as an input for the current process step. Unlike amnesiac FFNNs, this memory enables RNNs to process sequences of inputs without loosing track. The loops make it a recurrent network.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ededef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([doc])\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding/indexing\n",
    "sequences = tokenizer.texts_to_sequences([doc])[0]\n",
    "\n",
    "# Create input-output pairs for training\n",
    "X_train, Y_train = [], []\n",
    "seq_length = 3  # Number of words to predict the next word\n",
    "\n",
    "for i in range(len(sequences) - seq_length):\n",
    "    X_train.append(sequences[i:i+seq_length])\n",
    "    Y_train.append(sequences[i+seq_length])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = to_categorical(Y_train, num_classes=vocab_size)  # Convert output to one-hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c055ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MinLSTMCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.f_gate = tf.keras.layers.Dense(hidden_size, activation='sigmoid')\n",
    "        self.c_tilde = tf.keras.layers.Dense(hidden_size, activation=None)\n",
    "        self.output_layer = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "    def call(self, x_t, c_prev):\n",
    "        f_t = self.f_gate(x_t)                  # Forget gate from input only\n",
    "        c_hat = self.c_tilde(x_t)               # Candidate memory\n",
    "        c_t = f_t * c_prev + (1 - f_t) * c_hat  # Memory update (merged with hidden)\n",
    "        y_t = self.output_layer(c_t)            # Predict from current hidden state\n",
    "        return c_t, y_t                         # c_t is also h_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "988edcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_min_lstm(X, Y, vocab_size, hidden_size=10, embedding_dim=8, epochs=500):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    min_lstm = MinLSTMCell(hidden_size, vocab_size)\n",
    "\n",
    "    # Embedding layer to convert tokens to dense vectors\n",
    "    embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            c_prev = tf.zeros((X.shape[0], hidden_size), dtype=tf.float32)  # initial state\n",
    "\n",
    "            embedded_X = embedding_layer(X)  # [B, T, D]\n",
    "\n",
    "            # Feed sequence into the minLSTM one step at a time\n",
    "            for t in range(embedded_X.shape[1]):\n",
    "                x_t = embedded_X[:, t, :]\n",
    "                c_prev, y_t = min_lstm(x_t, c_prev)  # only use c_prev\n",
    "\n",
    "            # Crossentropy between predicted token distribution and ground truth\n",
    "            loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(Y, y_t))\n",
    "\n",
    "        grads = tape.gradient(loss, min_lstm.trainable_variables + embedding_layer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, min_lstm.trainable_variables + embedding_layer.trainable_variables))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy():.4f}\")\n",
    "\n",
    "        if epoch == epochs - 1:\n",
    "            return min_lstm, embedding_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25eeffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_min_lstm(model, tokenizer, input_text, embedding_layer, hidden_size):\n",
    "    seq_length = 3\n",
    "    sequence = tokenizer.texts_to_sequences([input_text])[0][-seq_length:]\n",
    "    X_input = np.array([sequence])\n",
    "    c_prev = tf.zeros((1, hidden_size), dtype=tf.float32)\n",
    "\n",
    "    embedded_X = embedding_layer(X_input)\n",
    "\n",
    "    for t in range(embedded_X.shape[1]):\n",
    "        x_t = embedded_X[:, t, :]\n",
    "        c_prev, y_t = model(x_t, c_prev)\n",
    "\n",
    "    predicted_index = tf.argmax(y_t, axis=1).numpy()[0]\n",
    "    return tokenizer.index_word.get(predicted_index, \"<UNK>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa4ab0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.7002\n",
      "Epoch 10, Loss: 5.4715\n",
      "Epoch 20, Loss: 4.8913\n",
      "Epoch 30, Loss: 4.3175\n",
      "Epoch 40, Loss: 3.6769\n",
      "Epoch 50, Loss: 3.0428\n",
      "Epoch 60, Loss: 2.4444\n",
      "Epoch 70, Loss: 1.9062\n",
      "Epoch 80, Loss: 1.4492\n",
      "Epoch 90, Loss: 1.0929\n",
      "Epoch 100, Loss: 0.8327\n",
      "Epoch 110, Loss: 0.6461\n",
      "Epoch 120, Loss: 0.5074\n",
      "Epoch 130, Loss: 0.4022\n",
      "Epoch 140, Loss: 0.3229\n",
      "Epoch 150, Loss: 0.2638\n",
      "Epoch 160, Loss: 0.2194\n",
      "Epoch 170, Loss: 0.1860\n",
      "Epoch 180, Loss: 0.1611\n",
      "Epoch 190, Loss: 0.1427\n",
      "Epoch 200, Loss: 0.1288\n",
      "Epoch 210, Loss: 0.1181\n",
      "Epoch 220, Loss: 0.1098\n",
      "Epoch 230, Loss: 0.1031\n",
      "Epoch 240, Loss: 0.0977\n",
      "Epoch 250, Loss: 0.0933\n",
      "Epoch 260, Loss: 0.0896\n",
      "Epoch 270, Loss: 0.0864\n",
      "Epoch 280, Loss: 0.0836\n",
      "Epoch 290, Loss: 0.0813\n",
      "Epoch 300, Loss: 0.0792\n",
      "Epoch 310, Loss: 0.0774\n",
      "Epoch 320, Loss: 0.0759\n",
      "Epoch 330, Loss: 0.0746\n",
      "Epoch 340, Loss: 0.0735\n",
      "Epoch 350, Loss: 0.0725\n",
      "Epoch 360, Loss: 0.0716\n",
      "Epoch 370, Loss: 0.0709\n",
      "Epoch 380, Loss: 0.0704\n",
      "Epoch 390, Loss: 0.0698\n",
      "Epoch 400, Loss: 0.0693\n",
      "Epoch 410, Loss: 0.0689\n",
      "Epoch 420, Loss: 0.0685\n",
      "Epoch 430, Loss: 0.0682\n",
      "Epoch 440, Loss: 0.0679\n",
      "Epoch 450, Loss: 0.0676\n",
      "Epoch 460, Loss: 0.0674\n",
      "Epoch 470, Loss: 0.0672\n",
      "Epoch 480, Loss: 0.0670\n",
      "Epoch 490, Loss: 0.0668\n",
      "Input: 'recurrent neural networks' → Predicted next word: 'are'\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "min_lstm_model, emb_layer = train_min_lstm(X_train, Y_train, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "405913f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'recurrent neural networks' → Predicted next word: 'are'\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "input_text = \"recurrent neural networks\"\n",
    "predicted_word = predict_min_lstm(min_lstm_model, tokenizer, input_text, emb_layer, hidden_size=10)\n",
    "print(f\"Input: '{input_text}' → Predicted next word: '{predicted_word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca22c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
